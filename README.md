# 📉 Dimensionality Reduction & Feature Selection in Machine Learning

This repository contains implementations of popular **dimensionality reduction** and **feature selection** techniques using Python. These methods are essential for reducing complexity, improving model performance, and preventing overfitting in Machine Learning workflows.

---

## 🧠 Techniques Covered

- **PCA (Principal Component Analysis)**  
  Reduces dimensionality by projecting data onto principal components that maximise variance.

- **Kernel PCA**  
  Extends PCA using kernel methods to handle non-linear feature spaces.

- **LDA (Linear Discriminant Analysis)**  
  Supervised dimensionality reduction maximising class separability.

- **Chi-Square (SelectKBest)**  
  Statistical feature selection method for categorical data.

---

## 📂 Files in This Repository

| File Name                        | Description |
|----------------------------------|-------------|
| `PCA.ipynb`                      | Implementation of Principal Component Analysis |
| `Kernel_PCA.ipynb`               | Non-linear dimensionality reduction using Kernel PCA |
| `Linear Discriminant Analysis.ipynb` | LDA for supervised dimensionality reduction |
| `Select K - ChiSquare.ipynb`     | Feature selection using Chi-Square test |
| `README.md`                      | Overview of this repository |

---


## 📜 Author
- Sai Vennela Yadavalli
---

## ✨ Quote
“Reduce dimensions, reveal insights.” 🔍✨
